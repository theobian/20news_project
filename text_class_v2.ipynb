{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, codecs, string, random\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import spacy, nltk, gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "dirnames = \"data/\"+os.listdir(\"data\")[0]\n",
    "dirnames += \"/\"+ os.listdir(dirnames)[0]+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "    dirnames = \"data/\"+os.listdir(\"data\")[0]\n",
    "    dirnames += \"/\" + os.listdir(dirnames)[0]+\"/\"\n",
    "    return dirnames, os.listdir(dirnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath, dirnames = get_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename):\n",
    "    savename = \"./\"+filename+\".pkl\"\n",
    "    data.to_pickle(savename)\n",
    "    return savename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(savename):\n",
    "    data = pd.read_pickle(savename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(savename):\n",
    "    os.remove(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_quick(data, filename, dirname, dirpath):\n",
    "    \n",
    "    with open(dirpath + dirname + \"/\" + filename) as current_file:\n",
    "        doc = current_file.read()\n",
    "        \n",
    "        main_label = dirname\n",
    "        labels = dirname.split(\".\")\n",
    "        file_id = current_file.name[-5:]\n",
    "        text = \"\".join(re.split(\"\\nLines: [0123456789]+\\n\", doc)[1:])\n",
    "        \n",
    "        data = data.append(pd.DataFrame(np.array([[main_label, labels, file_id, text]]),\\\n",
    "                                        columns=[\"main_label\",\"labels\",\"file_id\", \"text\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data, filename, dirname, dirpath):\n",
    "    with open(dirpath + dirname + \"/\" + filename) as current_file:\n",
    "        doc = current_file.read()\n",
    "        \n",
    "        main_label = dirname\n",
    "        labels = dirname.split(\".\")\n",
    "        file_id = current_file.name[-5:]\n",
    "        \n",
    "        newsgroups, subject, organization, date, n_lines = \"\",\"\",\"\",\"\",\"\"\n",
    "        if(re.search(\"Newsgroups: .*\\n\", doc) and re.findall(\"Newsgroups: .*\\n\", doc)[0]):\n",
    "            newsgroups = re.findall(\"Newsgroups: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Newsgroups: \")[-1].split(\",\")\n",
    "\n",
    "        if(re.search(\"Subject: .*\\n\", doc) and re.findall(\"Subject: .*\\n\", doc)[0]):\n",
    "            subject = re.findall(\"Subject: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Subject: \")[-1]\n",
    "\n",
    "        if(re.search(\"Organization: .*\\n\", doc) and re.findall(\"Organization: .*\\n\", doc)[0]):\n",
    "            organization = re.findall(\"Organization: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Organization: \")[-1]\n",
    "\n",
    "        if(re.search(\"Date: .*\\n\", doc) and re.findall(\"Date: .*\\n\", doc)[0]):\n",
    "            date = re.findall(\"Date: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Date: \")[-1]\n",
    "\n",
    "        if(re.search(\"Lines: .*\\n\", doc) and re.findall(\"Lines: .*\\n\", doc)[0]):\n",
    "            n_lines = re.findall(\"Lines: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Lines: \")[-1]\n",
    "        \n",
    "        text = \"\".join(re.split(\"\\nLines: [0123456789]+\\n\", doc)[1:])\n",
    "        \n",
    "        data = data.append(pd.DataFrame(np.array([[main_label, labels, file_id, newsgroups, subject, organization, date, n_lines, text]]),\\\n",
    "                                        columns=[\"main_label\",\"labels\",\"file_id\",\"newsgroups\",\"subject\",\"organization\",\"date\",\"n_lines\",\"text\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory_quick(dirnames, dirpath, data):\n",
    "    for filename in tqdm(os.listdir(dirpath+dirname)): \n",
    "        data = read_file_quick(data, filename, dirname, dirpath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_directory(dirnames, dirpath, data):\n",
    "    for filename in tqdm(os.listdir(dirpath+dirname)):  \n",
    "        data = read_file(data, filename, dirname, dirpath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame(columns = [\"main_label\", \"labels\",\"file_id\",\"newsgroups\",\"subject\",\"organization\",\"date\",\"n_lines\",\"text\"])\n",
    "# for dirname in tqdm_notebook(dirnames):\n",
    "#     data = read_directory(dirnames, dirpath, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8fb7ab49348e495e2ea2fbdfb0025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1085.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1084.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1035.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1009.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 985.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 901.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 872.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 903.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 818.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 729.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 766.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 752.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 669.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 658.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 649.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 997/997 [00:01<00:00, 653.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15997, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 622.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16997, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 600.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17997, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 567.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18997, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 518.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(columns = [\"main_label\",\"labels\",\"file_id\",\"text\"])\n",
    "for dirname in tqdm_notebook(dirnames):\n",
    "    data = read_directory_quick(dirnames, dirpath, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = save_data(data, \"quick_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./quick_data_1.pkl'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"quick_data_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_label</th>\n",
       "      <th>labels</th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[sci, electronics]</td>\n",
       "      <td>52723</td>\n",
       "      <td>\\ng92m3062@alpha.ru.ac.za (Brad Meier) writes:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        main_label              labels file_id  \\\n",
       "0  sci.electronics  [sci, electronics]   52723   \n",
       "\n",
       "                                                text  \n",
       "0  \\ng92m3062@alpha.ru.ac.za (Brad Meier) writes:...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = data.copy()\n",
    "temp= temp.iloc[12000:12010]\n",
    "temp.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_CHARS = list(set(punctuation).union(set('’')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and then lemmatize\n",
    "def lemmatizer(doc):\n",
    "    words = [token.lemma_ for token in nlp(doc) if token.is_stop != True \\\n",
    "             and token.is_punct != True \\\n",
    "             and token.is_space != True \\\n",
    "             and token.is_digit != True \\\n",
    "             and re.search(\"\\\\\"+\"|\\\\\".join(EXCLUDE_CHARS), token.lemma_) is None \\\n",
    "             and re.search(\"|\".join(\"0123456789\"), token.lemma_) is None]\n",
    "    return words\n",
    "\n",
    "def lemmatizer_continuous(doc):\n",
    "    words = [token.lemma_ for token in nlp(doc) if token.is_stop != True \\\n",
    "             and token.is_punct != True \\\n",
    "             and token.is_space != True \\\n",
    "             and token.is_digit != True \\\n",
    "             and re.search(\"\\\\\"+\"|\\\\\".join(EXCLUDE_CHARS), token.lemma_) is None \\\n",
    "             and re.search(\"|\".join(\"0123456789\"), token.lemma_) is None]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "# tqdm_notebook.pandas()\n",
    "# need to change pandas version to be able to import and use the progress_apply functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatizer_continuous: 1622.25 seconds for 10 lines\n",
      "lemmatizer: 1606.49 seconds for 10 lines\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "data[\"continuous\"] = data.text.apply(lambda x: lemmatizer_continuous(x))\n",
    "t2 = time.time()\n",
    "data[\"words\"] = data.text.apply(lambda x: lemmatizer(x))\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatizer_continuous: 1622.25 seconds  = 27.04 minutes for 19997 lines\n",
      "lemmatizer: 1606.49 seconds = 26.77 minutes for 19997 lines\n"
     ]
    }
   ],
   "source": [
    "print(\"lemmatizer_continuous: {:.2f} seconds  = {:.2f} minutes for {} lines\".format(t2-t1, (t2-t1)/60, len(data)))\n",
    "print(\"lemmatizer: {:.2f} seconds = {:.2f} minutes for {} lines\".format(t3-t2, (t3-t2)/60, len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./quick_data_processed_1.pkl'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data(data, \"quick_data_processed_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = load_data(\"quick_data_processed_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = Counter(data_processed.words.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 29407),\n",
       " ('not', 29401),\n",
       " ('the', 26935),\n",
       " ('write', 18496),\n",
       " ('in', 18098),\n",
       " ('article', 12943),\n",
       " ('know', 11543),\n",
       " ('good', 10416),\n",
       " ('like', 10379),\n",
       " ('people', 10328)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOOOh should remove some more stopwords. very common list is not specific. \n",
    "# should then use Luhn: remove most common and least commmon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should also have the dirname where it comes form\n",
    "# should check that we have roughly the same amount of datapoints in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "# join with other set and .unique()\n",
    "# also should remove numbers that are within words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import dropwhile, islice, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "newbag  = bag_of_words.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, count in islice(bag_of_words.most_common(),10):\n",
    "#     print(key, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage min is the percentage of words that are less frequent that you want to drop e.g. 10% less frequent\n",
    "# sorted from most frequent to least frequent:\n",
    "# so if you take most_common()[minimum value - maximum value]\n",
    "# you need to have minimum value = the most frequent you want = the maximum percentage. but you need to inverse the percentages\n",
    "# because if you want 90% less frequent words, and it is sorted by most frequent, you need to discard the top 10%\n",
    "# similarly if you want to remove the 10% less frequent you need to remove the slice from 90% to 100%\n",
    "# so use a nice 1-probability\n",
    "def remove_between_thresholds(bag_of_words, percentage_min, percentage_max):\n",
    "    t_min = int(len(bag_of_words)*(1-percentage_min/100))\n",
    "    t_max = int(len(bag_of_words)*(1-percentage_max/100))\n",
    "    return Counter(dict(bag_of_words.most_common()[t_max: t_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are based on the count....\n",
    "def remove_less_frequent(bag_of_words, threshold):\n",
    "    for key, count in dropwhile(lambda x: x[1] > threshold, bag_of_words.most_common()):\n",
    "        del bag_of_words[key]\n",
    "    return bag_of_words\n",
    "\n",
    "def remove_most_frequent(bag_of_words, threshold):\n",
    "    for key, count in takewhile(lambda x: x[1] < threshold, bag_of_words.most_common()):\n",
    "        del bag_of_words[key]\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_slim = remove_between_thresholds(bag_of_words, 5, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.fit(data.labels.explode().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at= temp.iloc[12000:12010]\n",
    "# at.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_labels(data, le):\n",
    "#     data[\"one_hot_labels\"] = data.apply(lambda x: le.transform(x.labels), axis = 1)\n",
    "#     return data\n",
    "# pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) → 'DataFrame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new.a.apply(lambda x: pd.get_dummies(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.transform(at.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a test\n",
    "# le.inverse_transform(le.transform([\"sci\", \"electronics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = le.transform(at.labels.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_label</th>\n",
       "      <th>labels</th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "      <th>continuous</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>[alt, atheism]</td>\n",
       "      <td>49960</td>\n",
       "      <td>\\nArchive-name: atheism/resources\\nAlt-atheism...</td>\n",
       "      <td>archive atheism resource alt atheism archive r...</td>\n",
       "      <td>[archive, atheism, resource, alt, atheism, arc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    main_label          labels file_id  \\\n",
       "0  alt.atheism  [alt, atheism]   49960   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nArchive-name: atheism/resources\\nAlt-atheism...   \n",
       "\n",
       "                                          continuous  \\\n",
       "0  archive atheism resource alt atheism archive r...   \n",
       "\n",
       "                                               words  \n",
       "0  [archive, atheism, resource, alt, atheism, arc...  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_processed.continuous\n",
    "y = data_processed.main_label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "mlb.fit(y_train)\n",
    "tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17997, 71038)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17997, 71038)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17997,)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb =  MultinomialNB()\n",
    "mnb.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_count = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8215\n"
     ]
    }
   ],
   "source": [
    "mnb_predictions= mnb.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, mnb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth = None, n_estimators = 150, random_state = 0)\n",
    "rfc.fit(X_train_tfidf, mlb.transform(y_train))\n",
    "rfc_predictions = rfc.predict(X_test_tfidf)\n",
    "print(accuracy_score(mlb.transform(y_test), rfc_predictions))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we know how to predict the \"main\" label but its consistently wrong...\n",
    "# now on to the other issues: predict labels individually\n",
    "# shoudl also look at different methods\n",
    "# can try to use only the count vectorizer\n",
    "# can try to predict mulitiple labels\n",
    "# can do sentiment analysis\n",
    "# should try other classifiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
