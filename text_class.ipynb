{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, codecs, string, random\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import spacy, nltk, gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirnames = \"data/\"+os.listdir(\"data\")[0]\n",
    "dirnames += \"/\"+ os.listdir(dirnames)[0]+\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "let's get a look at all the different types of documents we will be dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dirnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame()\n",
    "# for dirname in tqdm(os.listdir(dirnames)):\n",
    "#     print(dirname)\n",
    "#     for file in os.listdir(dirnames + dirname):\n",
    "#         doc = open(file, 'r', errors='ignore').read()\n",
    "#         data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather some information: \n",
    "#      if not empty: newsgroups, subject, organization, date, lines\n",
    "# split on :\"Lines: \"\n",
    "\n",
    "# separate file into \"lines\" with separator \">\" si useless\n",
    "# remove any line containing @ : split on \"/n\" and remove if in\n",
    "# remove stopwords\n",
    "# remove any empty \"line\"\n",
    "# lowercase but watch out for names\n",
    "# for each folder / category: separate word counts\n",
    "# merge all into global word count while keeping the others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should make it into a reading function of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "atheism = pd.DataFrame(columns = [\"id\",\"newsgroups\",\"subject\",\"organization\",\"date\",\"n_lines\",\"text\"])\n",
    "dirname = dirnames + os.listdir(dirnames)[0] + \"/\"\n",
    "for filename in os.listdir(dirname):\n",
    "    file = dirname + filename\n",
    "    with open(file) as current_file:\n",
    "#         should check that all files have the same number of digits in the IID :////\n",
    "        id = current_file.name[-5:]\n",
    "        doc = current_file.read()\n",
    "        if(re.search(\"Newsgroups: .*\\n\", doc) and re.findall(\"Newsgroups: .*\\n\", doc)[0]):\n",
    "            newsgroups = re.findall(\"Newsgroups: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Newsgroups: \")[-1].split(\",\")\n",
    "            \n",
    "        if(re.search(\"Subject: .*\\n\", doc) and re.findall(\"Subject: .*\\n\", doc)[0]):\n",
    "            subject = re.findall(\"Subject: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Subject: \")[-1]\n",
    "            \n",
    "        if(re.search(\"Organization: .*\\n\", doc) and re.findall(\"Organization: .*\\n\", doc)[0]):\n",
    "            organization = re.findall(\"Organization: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Organization: \")[-1]\n",
    "            \n",
    "        if(re.search(\"Date: .*\\n\", doc) and re.findall(\"Date: .*\\n\", doc)[0]):\n",
    "            date = re.findall(\"Date: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Date: \")[-1]\n",
    "            \n",
    "        if(re.search(\"Lines: .*\\n\", doc) and re.findall(\"Lines: .*\\n\", doc)[0]):\n",
    "            n_lines = re.findall(\"Lines: .*\\n\", doc)[0]\\\n",
    "                            .split(\"\\n\")[0].split(\"Lines: \")[-1]\n",
    "            \n",
    "    #         make sure it exists\n",
    "#         text = \"\".join(re.split(\".*@.*\",\"\".join(re.split(\"\\nLines: [0123456789]+\\n\\n\", doc)[1:])))\n",
    "# were saying that there always exists these stuff,,, basically we'll drop anything not having these\n",
    "# since we're taking the first split to be the preamble, if there is no first preamble split but one later, it will\n",
    "# remove part of the text, but we will say that this doesnt happen too often\n",
    "#  also basically we're saying that \"Lines\" is a stopword! since we're splitting on it :/ \n",
    "# we could split and add it back but it makes sense to keep it as a stopword in most cases except geometry / else.... \n",
    "# but at the same time, looking at the list of stopwords we are going to use later, there are words such as \n",
    "# \"keep\" which has th same issue! so we're fine.\n",
    "        text = \"\".join(re.split(\"\\nLines: [0123456789]+\\n\", doc)[1:])\n",
    "        atheism = atheism.append(pd.DataFrame(np.array([[id, newsgroups, subject, organization, date, n_lines, text]]),\\\n",
    "                                    columns=[\"id\",\"newsgroups\",\"subject\",\"organization\",\"date\",\"n_lines\",\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = atheism.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>newsgroups</th>\n",
       "      <th>subject</th>\n",
       "      <th>organization</th>\n",
       "      <th>date</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49960</td>\n",
       "      <td>[alt.atheism, alt.atheism.moderated, news.answ...</td>\n",
       "      <td>Alt.Atheism FAQ: Atheist Resources</td>\n",
       "      <td>Mantis Consultants, Cambridge. UK.</td>\n",
       "      <td>Mon, 29 Mar 1993 11:57:19 GMT</td>\n",
       "      <td>290</td>\n",
       "      <td>\\nArchive-name: atheism/resources\\nAlt-atheism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51060</td>\n",
       "      <td>[alt.atheism, alt.atheism.moderated, news.answ...</td>\n",
       "      <td>Alt.Atheism FAQ: Introduction to Atheism</td>\n",
       "      <td>Mantis Consultants, Cambridge. UK.</td>\n",
       "      <td>Mon, 5 Apr 1993 12:22:45 GMT</td>\n",
       "      <td>646</td>\n",
       "      <td>\\nArchive-name: atheism/introduction\\nAlt-athe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51119</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: Gospel Dating</td>\n",
       "      <td>Technical University Braunschweig, Germany</td>\n",
       "      <td>Mon, 5 Apr 1993 19:08:25 GMT</td>\n",
       "      <td>93</td>\n",
       "      <td>\\nIn article &lt;65974@mimsy.umd.edu&gt;\\nmangoe@cs....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51120</td>\n",
       "      <td>[alt.atheism, alt.politics.usa.constitution]</td>\n",
       "      <td>Re: university violating separation of church/...</td>\n",
       "      <td>Mantis Consultants, Cambridge. UK.</td>\n",
       "      <td>Mon, 5 Apr 1993 17:58:42 GMT</td>\n",
       "      <td>29</td>\n",
       "      <td>\\ndmn@kepler.unh.edu (...until kings become ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51121</td>\n",
       "      <td>[alt.atheism, soc.motss, rec.scouting]</td>\n",
       "      <td>Re: [soc.motss, et al.] \"Princeton axes matchi...</td>\n",
       "      <td>IBM Research</td>\n",
       "      <td>Mon, 05 Apr 93 18:01:16 GMT</td>\n",
       "      <td>15</td>\n",
       "      <td>\\nIn article &lt;N4HY.93Apr5120934@harder.ccr-p.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51122</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: A visit from the Jehovah's Witnesses</td>\n",
       "      <td>Technical University Braunschweig, Germany</td>\n",
       "      <td>Mon, 5 Apr 1993 19:24:19 GMT</td>\n",
       "      <td>114</td>\n",
       "      <td>\\nIn article &lt;1993Apr5.091139.823@batman.bmd.t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51123</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: Political Atheists?</td>\n",
       "      <td>California Institute of Technology, Pasadena</td>\n",
       "      <td>2 Apr 93 19:05:57 GMT</td>\n",
       "      <td>11</td>\n",
       "      <td>NNTP-Posting-Host: punisher.caltech.edu\\n\\narr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51124</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: An Anecdote about Islam</td>\n",
       "      <td>Technical University Braunschweig, Germany</td>\n",
       "      <td>Mon, 5 Apr 1993 19:45:34 GMT</td>\n",
       "      <td>28</td>\n",
       "      <td>\\nIn article &lt;114127@bu.edu&gt;\\njaeger@buphy.bu....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51125</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: &lt;Political Atheists?</td>\n",
       "      <td>California Institute of Technology, Pasadena</td>\n",
       "      <td>2 Apr 1993 20:43:17 GMT</td>\n",
       "      <td>54</td>\n",
       "      <td>Message-ID: &lt;1pi8h5INNq40@gap.caltech.edu&gt;\\nRe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51126</td>\n",
       "      <td>[alt.atheism]</td>\n",
       "      <td>Re: &gt;&gt;&gt;&gt;&gt;&gt;Pompous ass</td>\n",
       "      <td>California Institute of Technology, Pasadena</td>\n",
       "      <td>2 Apr 93 20:57:33 GMT</td>\n",
       "      <td>9</td>\n",
       "      <td>NNTP-Posting-Host: punisher.caltech.edu\\n\\nkmr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         newsgroups  \\\n",
       "0  49960  [alt.atheism, alt.atheism.moderated, news.answ...   \n",
       "0  51060  [alt.atheism, alt.atheism.moderated, news.answ...   \n",
       "0  51119                                      [alt.atheism]   \n",
       "0  51120       [alt.atheism, alt.politics.usa.constitution]   \n",
       "0  51121             [alt.atheism, soc.motss, rec.scouting]   \n",
       "0  51122                                      [alt.atheism]   \n",
       "0  51123                                      [alt.atheism]   \n",
       "0  51124                                      [alt.atheism]   \n",
       "0  51125                                      [alt.atheism]   \n",
       "0  51126                                      [alt.atheism]   \n",
       "\n",
       "                                             subject  \\\n",
       "0                 Alt.Atheism FAQ: Atheist Resources   \n",
       "0           Alt.Atheism FAQ: Introduction to Atheism   \n",
       "0                                  Re: Gospel Dating   \n",
       "0  Re: university violating separation of church/...   \n",
       "0  Re: [soc.motss, et al.] \"Princeton axes matchi...   \n",
       "0           Re: A visit from the Jehovah's Witnesses   \n",
       "0                            Re: Political Atheists?   \n",
       "0                        Re: An Anecdote about Islam   \n",
       "0                           Re: <Political Atheists?   \n",
       "0                              Re: >>>>>>Pompous ass   \n",
       "\n",
       "                                   organization  \\\n",
       "0            Mantis Consultants, Cambridge. UK.   \n",
       "0            Mantis Consultants, Cambridge. UK.   \n",
       "0    Technical University Braunschweig, Germany   \n",
       "0            Mantis Consultants, Cambridge. UK.   \n",
       "0                                  IBM Research   \n",
       "0    Technical University Braunschweig, Germany   \n",
       "0  California Institute of Technology, Pasadena   \n",
       "0    Technical University Braunschweig, Germany   \n",
       "0  California Institute of Technology, Pasadena   \n",
       "0  California Institute of Technology, Pasadena   \n",
       "\n",
       "                            date n_lines  \\\n",
       "0  Mon, 29 Mar 1993 11:57:19 GMT     290   \n",
       "0   Mon, 5 Apr 1993 12:22:45 GMT     646   \n",
       "0   Mon, 5 Apr 1993 19:08:25 GMT      93   \n",
       "0   Mon, 5 Apr 1993 17:58:42 GMT      29   \n",
       "0    Mon, 05 Apr 93 18:01:16 GMT      15   \n",
       "0   Mon, 5 Apr 1993 19:24:19 GMT     114   \n",
       "0          2 Apr 93 19:05:57 GMT      11   \n",
       "0   Mon, 5 Apr 1993 19:45:34 GMT      28   \n",
       "0        2 Apr 1993 20:43:17 GMT      54   \n",
       "0          2 Apr 93 20:57:33 GMT       9   \n",
       "\n",
       "                                                text  \n",
       "0  \\nArchive-name: atheism/resources\\nAlt-atheism...  \n",
       "0  \\nArchive-name: atheism/introduction\\nAlt-athe...  \n",
       "0  \\nIn article <65974@mimsy.umd.edu>\\nmangoe@cs....  \n",
       "0  \\ndmn@kepler.unh.edu (...until kings become ph...  \n",
       "0  \\nIn article <N4HY.93Apr5120934@harder.ccr-p.i...  \n",
       "0  \\nIn article <1993Apr5.091139.823@batman.bmd.t...  \n",
       "0  NNTP-Posting-Host: punisher.caltech.edu\\n\\narr...  \n",
       "0  \\nIn article <114127@bu.edu>\\njaeger@buphy.bu....  \n",
       "0  Message-ID: <1pi8h5INNq40@gap.caltech.edu>\\nRe...  \n",
       "0  NNTP-Posting-Host: punisher.caltech.edu\\n\\nkmr...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atheism.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atheism.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if needed\n",
    "example = nlp(atheism.iloc[0,5])[0:100]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('290', '290', 'NUM')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.lemma_, token.pos_) for token in example][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_CHARS = list(set(punctuation).union(set('’')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ - ! . ’ ( | % + , * \" ) ] & \\' ; > ` < _ $ } ? @ = \\\\ { # : ~ / ^'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(EXCLUDE_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and then lemmatize\n",
    "def lemmatizer(doc):\n",
    "    words = [token.lemma_ for token in nlp(doc) if token.is_stop != True \\\n",
    "             and token.is_punct != True \\\n",
    "             and token.is_space != True \\\n",
    "             and re.search(\"\\\\\"+\"|\\\\\".join(EXCLUDE_CHARS), token.lemma_) is None]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still getting some weird ones, so: remove \">\", so should find a list of special characters to escape\n",
    "# we re not doing name bundling and we're not taking care of proper nouns / etc, for another time\n",
    "#     not lemmatized properly: still gettin some ve , s etc\n",
    "#         not all lower cased\n",
    "#         remove any word that contains no space but @ or \".\" in the middle of it\n",
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f temp\n",
    "temp = atheism.copy(deep = True)\n",
    "temp = temp.iloc[0:10]\n",
    "# to test out stuff first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for notebooks\n",
    "from tqdm import tqdm, tqdm_pandas, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                         | 0/2442 [00:00<?, ?it/s]\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2442/2442 [00:00<00:00, 90264.47it/s]\n",
      "\n",
      "  0%|                                                                                         | 0/6805 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 6805/6805 [00:00<00:00, 117931.93it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/950 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 950/950 [00:00<00:00, 57261.36it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/319 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 319/319 [00:00<00:00, 43911.49it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/131 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 131/131 [00:00<00:00, 24542.34it/s]\n",
      "\n",
      "  0%|                                                                                         | 0/1157 [00:00<?, ?it/s]\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1157/1157 [00:00<00:00, 98666.43it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/106 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 106/106 [00:00<00:00, 21611.72it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/298 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 298/298 [00:00<00:00, 63008.65it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/650 [00:00<?, ?it/s]\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 650/650 [00:00<00:00, 61385.13it/s]\n",
      "\n",
      "  0%|                                                                                           | 0/64 [00:00<?, ?it/s]\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:00<00:00, 14451.44it/s]"
     ]
    }
   ],
   "source": [
    "# do this on the full doc \n",
    "temp.text = temp.text.apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer works ok,\n",
    "# now need to check the no data text rows, not normal.\n",
    "# but is a slow method!.... maybe should do this when reading? probably not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should remove tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  do this on the full doc \n",
    "\n",
    "bag_of_words = Counter(temp.text.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atheist', 100),\n",
       " ('god', 92),\n",
       " ('not', 67),\n",
       " ('the', 58),\n",
       " ('atheism', 57),\n",
       " ('be', 52),\n",
       " ('religion', 50),\n",
       " ('religious', 47),\n",
       " ('believe', 41),\n",
       " ('belief', 39)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should add some characters to the stop words list: \"not\" \"the\" and some others.... should check the stopword list we are using:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should also implement a tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could filter out using Luhn or at least remove hapaxeS? lets see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "could also train a ngram probabilistic parser? but no truth :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now on to the non processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do sentiment analysis?\n",
    "# could predict the tag / category ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
